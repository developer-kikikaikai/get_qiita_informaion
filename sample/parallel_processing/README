## 並列処理での5000件の記事を一気に取得

### 使い方:
各jsonのaccess_token部分
"aaaaaaaaaa"
を自分のものに変更後、parallel.pyを実行してください。

変換例(Linux)

```
ls *.json | xargs sed -i "s/aaaaaaaaaa/write_your_access_token_here/g"
```

今回はitem, title, タグ, いいね数がファイルに出力されます。
"xxx":true(views、ストック数、コメント数以外)を増やせば情報量ももっと増えます。

### 出力:
ファイルpage_{index}_plus_10.txtに、1000, 2000, 3000, 4000, 5000番目の記事が出力されます。
(命名がいまいちですね(笑))

普通に準繰り取得していくよりも遥かに速いです。
計測していないけど1分以上⇒12,3秒って感じでした。

### その他

仕組みについて解説。ページネーションの仕組みのpageで指定されるのは、per_pageで記事数を区切ったうちの何ページ目から取得するか？という意味になります。
最新1000個の記事でper_page=100だったら順番としては1pageから数えると1, 101, 201, 301,...番目の記事が取得対象になります。
なので、例えば1プロセス1000記事を取得するなら、1page, 11page, 21pageと取得開始位置をずらして並列に情報取得を行えば、重複なく大量の記事を取得できるという感じです。

### 必要環境

python3以上 (3.6で動作確認)

Ubuntuでの環境インストール方法

```
sudo apt install python3
```
